import streamlit as st
import os

from langchain_community.document_loaders import Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ 
from langchain_upstage import UpstageEmbeddings
from langchain_chroma import Chroma
from langchain import hub
from langchain.chains import RetrievalQA
from langchain_upstage import ChatUpstage

st.set_page_config(page_title="ì˜ë£Œë²• ì±—ë´‡", page_icon="ğŸ’»")

st.title("ğŸ’»ì˜ë£Œë²• ì±—ë´‡")
st.caption("ì˜ë£Œë²•ì— ê´€ë ¨ëœ ëª¨ë“  ê²ƒì„ ë‹µí•´ë“œë¦½ë‹ˆë‹¤.")

load_dotenv()

api_key = os.getenv("UPSTAGE_API_KEY")

# ë°ì´í„° ë¡œë“œ ë° ë¬¸ì„œ ë¶„ë¦¬ 
loader = Docx2txtLoader('./medical.docx')
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
document_list = loader.load_and_split(text_splitter=text_splitter)

# Embedding ìƒì„±
embedding = UpstageEmbeddings(model="embedding_query")

# # Chroma ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” 
# database = Chroma.from_documents(
#     document= document_list,
#     embedding=embedding,
#     collection_name='medical_law',
#     persist_directory='./chroma'
# )

persist_directory = "./chroma"

# ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒì„±
if os.path.exists(persist_directory):
    # ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
    database = Chroma(
        collection_name='medical-law',
        persist_directory=persist_directory,
        embedding_function=embedding
    )
else:
    # ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
    database = Chroma.from_documents(
        documents=document_list,
        embedding=embedding,
        collection_name='medical-law',
        persist_directory=persist_directory
    )

# LLM ë° í”„ë¡¬í”„íŠ¸ ì„¤ì •
llm = ChatUpstage(model="solar-pro", api_key=api_key)
prompt = hub.pull("rlm/rag-prompt")

# RetrievalQA ì²´ì¸
qa_chain = RetrievalQA.from_chain_type(
    llm, 
    retriever = database.as_retriever(),
    chain_type_kwargs = {"prompt": prompt}
)

# AI ë‹µë³€ ìƒì„± í•¨ìˆ˜
def get_ai_message(user_message):
    ai_response = qa_chain.invoke({"query": user_message})
    return ai_response["result"]

if 'message_list' not in st.session_state:
    st.session_state.message_list = []

# ì´ì „ ë©”ì‹œì§€ í‘œì‹œ 
for message in st.session_state.message_list:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if user_question := st.chat_input(placeholder="ì˜ë£Œë²•ì— ê´€ë ¨ëœ ê¶ê¸ˆí•œ ë‚´ìš©ë“¤ì„ ë§ì”€í•´ì£¼ì„¸ìš”!"):
    with st.chat_message("user"):
        st.write(user_question)
    st.session_state.message_list.append({"role": "user", "content": user_question})

    # AI ì‘ë‹µ ìƒì„± ë° í‘œì‹œ 
    with st.chat_message("ai"):
        ai_message = get_ai_message(user_question)
        st.write(ai_message)
    st.session_state.message_list.append({"role": "ai", "content": ai_message})





